{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1.\tGenerator: Combines a linear mapping from a latent space (z) with a transformer encoder to capture temporal relationships between video frames. Uses deconvolution layers to upscale embeddings into high-resolution video frames.\n",
    "# 2.\tDiscriminator: Processes each frame with convolutional layers.\n",
    "# o\tAggregates features across frames to evaluate both spatial quality and temporal consistency. Uses an adversarial training loop with a loss function (e.g., GAN loss) to train both the generator and discriminator.\n",
    "\n",
    "# Future Goal:\tUsing a more sophisticated transformer architecture (e.g., attention masking for sequence positions). Adding perceptual or temporal loss for enhanced video realism. Incorporating motion dynamics models for better temporal coherence.\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, latent_dim, seq_len, embed_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Linear mapping for latent vector to sequence embedding\n",
    "        self.fc = nn.Linear(latent_dim, seq_len * embed_dim)\n",
    "\n",
    "        # Transformer Encoder for temporal consistency\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=8, dim_feedforward=512)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "        # Deconvolution layers for upscaling to high-resolution video frames\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embed_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Map latent vector to sequence embedding\n",
    "        batch_size = z.size(0)\n",
    "        sequence_embed = self.fc(z).view(batch_size, self.seq_len, self.embed_dim)\n",
    "\n",
    "        # Apply transformer to ensure temporal consistency\n",
    "        temporal_output = self.transformer(sequence_embed)\n",
    "\n",
    "        # Reshape and pass through deconvolution layers for video frame generation\n",
    "        frames = temporal_output.view(batch_size * self.seq_len, self.embed_dim, 1, 1)\n",
    "        high_res_frames = self.deconv(frames).view(batch_size, self.seq_len, -1, 64, 64)  # Example: 64x64 resolution\n",
    "        return high_res_frames\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, seq_len):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Convolution layers for frame-level feature extraction\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        # Linear layer for temporal consistency and classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8 * seq_len, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, video_frames):\n",
    "        batch_size = video_frames.size(0)\n",
    "        seq_len = video_frames.size(1)\n",
    "\n",
    "        # Extract features for each frame and concatenate\n",
    "        features = []\n",
    "        for t in range(seq_len):\n",
    "            frame_features = self.conv(video_frames[:, t, :, :, :])\n",
    "            features.append(frame_features.view(batch_size, -1))\n",
    "        \n",
    "        temporal_features = torch.cat(features, dim=1)\n",
    "        output = self.fc(temporal_features)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "img_channels = 3\n",
    "latent_dim = 128\n",
    "seq_len = 16\n",
    "embed_dim = 256\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(img_channels, latent_dim, seq_len, embed_dim)\n",
    "discriminator = Discriminator(img_channels, seq_len)\n",
    "\n",
    "# Example latent vector\n",
    "z = torch.randn(4, latent_dim)  # Batch size of 4\n",
    "generated_video = generator(z)  # Generated video output\n",
    "print(\"Generated video shape:\", generated_video.shape)  # [Batch, Seq, Channels, Height, Width]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
